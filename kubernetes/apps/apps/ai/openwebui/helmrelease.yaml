apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: openwebui
  namespace: ai
spec:
  chart:
    spec:
      chart: app-template
  values:
    controllers:
      main:
        containers:
          main:
            image:
              # renovate: datasource=github-tags depName=open-webui/open-webui
              repository: ghcr.io/open-webui/open-webui
              tag: 0.8.3
            env:
              TZ: Europe/Berlin
              ENABLE_PERSISTENT_CONFIG: false
              OPENAI_API_BASE_URLS: http://llama-swap:8080/v1
              ENABLE_OLLAMA_API: false
            ports:
              - name: http
                containerPort: 8080
            resources:
              requests:
                cpu: 100m
                memory: 512Mi
              limits:
                cpu: 1000m
                memory: 2Gi
    service:
      main:
        controller: main
        ports:
          http:
            port: 8080
    ingress:
      main:
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: traefik-gatekeeper-auth-chain@kubernetescrd
          external-dns.alpha.kubernetes.io/hostname: chat.${CLUSTER_DOMAIN}
          gethomepage.dev/name: Open WebUI
          gethomepage.dev/description: Chat UI for LLMs
          gethomepage.dev/group: AI
          gethomepage.dev/icon: open-webui.png
          gethomepage.dev/enabled: "true"
        hosts:
          - host: chat.${CLUSTER_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: main
                  port: http
    persistence:
      data:
        enabled: true
        type: persistentVolumeClaim
        accessMode: ReadWriteOnce
        size: 1Gi
        storageClass: longhorn-r1
        globalMounts:
          - path: /app/backend/data
